{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f09550b-a697-415c-8dfd-5d8347c807a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0bf9199-afb5-4ed1-8842-4942e0278e5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/tungks/gttm/reseach'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a8b8d66-6cc6-443a-8e38-f750206692eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "checkpoint = torch.load('../runs/detect/train3/weights/best.pt')\n",
    "model = checkpoint['model'].float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84fb2298-7a31-48ce-a212-020b74777da6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['epoch', 'best_fitness', 'model', 'ema', 'updates', 'optimizer', 'train_args', 'train_metrics', 'train_results', 'date', 'version', 'license', 'docs'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e45f816c-2428-4745-94b7-bb53bfb84739",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f149139e-0f63-4a7b-b74c-bf342a656c30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 9, 8400])\n"
     ]
    }
   ],
   "source": [
    "image = Image.open('/home/tungks/shared_drive_cv/GTTMData/external/traffic_camera/train/images/aguanambi-1000_png_jpg.rf.0ab6f274892b9b370e6441886b2d7b9d.jpg')\n",
    "image = np.array(image) \n",
    "image = image / 255.0 \n",
    "image = np.transpose(image, (2, 0, 1))  # Transpose to (channels, height, width)\n",
    "image = np.expand_dims(image, axis=0)  # Add batch dimension\n",
    "image = image.astype(np.float32)\n",
    "\n",
    "input_tensor = torch.from_numpy(image)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(input_tensor)\n",
    "\n",
    "print(output[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdbe8a49-4a26-4db0-9fdd-02d83aca82f2",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d2aaa34-5a79-4c0f-9886-35a49f78084b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea232e90-6545-4fe9-8fc8-13a7b9b070e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pretrained YOLOv8n model\n",
    "model = YOLO('yolov8n.pt')\n",
    "\n",
    "# Define path to video file\n",
    "source = '/raid/tungks/shared_drive_cv/GTTMData/external/youtube/videos/nga-tu-so_cutted.mp4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c754073-3b08-4a9c-9d85-e22a35ab8112",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object BasePredictor.stream_inference at 0x7fcf676eef90>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run inference on the source\n",
    "results = model(source, stream=True)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "820333ba-6367-401b-94f3-6509300220e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/minhnh/python_venv/cv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "video 1/1 (frame 1/12093) /raid/tungks/shared_drive_cv/GTTMData/external/youtube/videos/nga-tu-so_cutted.mp4: 384x640 13 persons, 15 cars, 5 motorcycles, 2 buss, 96.2ms\n",
      "ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: None\n",
      "masks: None\n",
      "names: {0: 'person', 1: 'bicycle', 2: 'car', 3: 'motorcycle', 4: 'airplane', 5: 'bus', 6: 'train', 7: 'truck', 8: 'boat', 9: 'traffic light', 10: 'fire hydrant', 11: 'stop sign', 12: 'parking meter', 13: 'bench', 14: 'bird', 15: 'cat', 16: 'dog', 17: 'horse', 18: 'sheep', 19: 'cow', 20: 'elephant', 21: 'bear', 22: 'zebra', 23: 'giraffe', 24: 'backpack', 25: 'umbrella', 26: 'handbag', 27: 'tie', 28: 'suitcase', 29: 'frisbee', 30: 'skis', 31: 'snowboard', 32: 'sports ball', 33: 'kite', 34: 'baseball bat', 35: 'baseball glove', 36: 'skateboard', 37: 'surfboard', 38: 'tennis racket', 39: 'bottle', 40: 'wine glass', 41: 'cup', 42: 'fork', 43: 'knife', 44: 'spoon', 45: 'bowl', 46: 'banana', 47: 'apple', 48: 'sandwich', 49: 'orange', 50: 'broccoli', 51: 'carrot', 52: 'hot dog', 53: 'pizza', 54: 'donut', 55: 'cake', 56: 'chair', 57: 'couch', 58: 'potted plant', 59: 'bed', 60: 'dining table', 61: 'toilet', 62: 'tv', 63: 'laptop', 64: 'mouse', 65: 'remote', 66: 'keyboard', 67: 'cell phone', 68: 'microwave', 69: 'oven', 70: 'toaster', 71: 'sink', 72: 'refrigerator', 73: 'book', 74: 'clock', 75: 'vase', 76: 'scissors', 77: 'teddy bear', 78: 'hair drier', 79: 'toothbrush'}\n",
      "obb: None\n",
      "orig_img: array([[[130,  83,  78],\n",
      "        [130,  83,  78],\n",
      "        [130,  83,  78],\n",
      "        ...,\n",
      "        [120,  90,  77],\n",
      "        [122,  94,  81],\n",
      "        [133, 105,  92]],\n",
      "\n",
      "       [[130,  83,  78],\n",
      "        [130,  83,  78],\n",
      "        [130,  83,  78],\n",
      "        ...,\n",
      "        [120,  90,  77],\n",
      "        [166, 138, 125],\n",
      "        [175, 147, 134]],\n",
      "\n",
      "       [[130,  83,  78],\n",
      "        [130,  83,  78],\n",
      "        [130,  83,  78],\n",
      "        ...,\n",
      "        [153, 133, 130],\n",
      "        [204, 186, 183],\n",
      "        [173, 155, 152]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[ 53,  62,  56],\n",
      "        [ 54,  63,  57],\n",
      "        [ 58,  67,  61],\n",
      "        ...,\n",
      "        [116, 117,  99],\n",
      "        [ 82,  83,  65],\n",
      "        [ 76,  77,  59]],\n",
      "\n",
      "       [[ 40,  49,  43],\n",
      "        [ 41,  50,  44],\n",
      "        [ 48,  57,  51],\n",
      "        ...,\n",
      "        [132, 135, 110],\n",
      "        [127, 129, 108],\n",
      "        [112, 114,  93]],\n",
      "\n",
      "       [[ 54,  63,  57],\n",
      "        [ 54,  63,  57],\n",
      "        [ 50,  59,  53],\n",
      "        ...,\n",
      "        [114, 117,  92],\n",
      "        [136, 138, 117],\n",
      "        [120, 122, 101]]], dtype=uint8)\n",
      "orig_shape: (720, 1280)\n",
      "path: '/raid/tungks/shared_drive_cv/GTTMData/external/youtube/videos/nga-tu-so_cutted.mp4'\n",
      "probs: None\n",
      "save_dir: None\n",
      "speed: {'preprocess': None, 'inference': None, 'postprocess': None}\n"
     ]
    }
   ],
   "source": [
    "for r in results:\n",
    "    print(r[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "395a42aa-e876-4f68-858c-98d366251777",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "video 1/1 (frame 289/12093) /raid/tungks/shared_drive_cv/GTTMData/external/youtube/videos/nga-tu-so_cutted.mp4: 384x640 8 persons, 15 cars, 2 motorcycles, 2 buss, 2 trucks, 5.5ms\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Results' object has no attribute 'frame'. See valid attributes below.\n\n    A class for storing and manipulating inference results.\n\n    Attributes:\n        orig_img (numpy.ndarray): Original image as a numpy array.\n        orig_shape (tuple): Original image shape in (height, width) format.\n        boxes (Boxes, optional): Object containing detection bounding boxes.\n        masks (Masks, optional): Object containing detection masks.\n        probs (Probs, optional): Object containing class probabilities for classification tasks.\n        keypoints (Keypoints, optional): Object containing detected keypoints for each object.\n        speed (dict): Dictionary of preprocess, inference, and postprocess speeds (ms/image).\n        names (dict): Dictionary of class names.\n        path (str): Path to the image file.\n\n    Methods:\n        update(boxes=None, masks=None, probs=None, obb=None): Updates object attributes with new detection results.\n        cpu(): Returns a copy of the Results object with all tensors on CPU memory.\n        numpy(): Returns a copy of the Results object with all tensors as numpy arrays.\n        cuda(): Returns a copy of the Results object with all tensors on GPU memory.\n        to(*args, **kwargs): Returns a copy of the Results object with tensors on a specified device and dtype.\n        new(): Returns a new Results object with the same image, path, and names.\n        plot(...): Plots detection results on an input image, returning an annotated image.\n        show(): Show annotated results to screen.\n        save(filename): Save annotated results to file.\n        verbose(): Returns a log string for each task, detailing detections and classifications.\n        save_txt(txt_file, save_conf=False): Saves detection results to a text file.\n        save_crop(save_dir, file_name=Path(\"im.jpg\")): Saves cropped detection images.\n        tojson(normalize=False): Converts detection results to JSON format.\n    ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m5\u001b[39m:\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mframe\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# boxes = result.boxes  # Boxes object for bounding box outputs\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# masks = result.masks  # Masks object for segmentation masks outputs\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# keypoints = result.keypoints  # Keypoints object for pose outputs\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# print('boxes \\n')\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# print(boxes.data)\u001b[39;00m\n\u001b[1;32m     15\u001b[0m i \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/home/minhnh/python_venv/cv/lib/python3.9/site-packages/ultralytics/utils/__init__.py:160\u001b[0m, in \u001b[0;36mSimpleClass.__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Custom attribute access error message with helpful information.\"\"\"\u001b[39;00m\n\u001b[1;32m    159\u001b[0m name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[0;32m--> 160\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. See valid attributes below.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__doc__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Results' object has no attribute 'frame'. See valid attributes below.\n\n    A class for storing and manipulating inference results.\n\n    Attributes:\n        orig_img (numpy.ndarray): Original image as a numpy array.\n        orig_shape (tuple): Original image shape in (height, width) format.\n        boxes (Boxes, optional): Object containing detection bounding boxes.\n        masks (Masks, optional): Object containing detection masks.\n        probs (Probs, optional): Object containing class probabilities for classification tasks.\n        keypoints (Keypoints, optional): Object containing detected keypoints for each object.\n        speed (dict): Dictionary of preprocess, inference, and postprocess speeds (ms/image).\n        names (dict): Dictionary of class names.\n        path (str): Path to the image file.\n\n    Methods:\n        update(boxes=None, masks=None, probs=None, obb=None): Updates object attributes with new detection results.\n        cpu(): Returns a copy of the Results object with all tensors on CPU memory.\n        numpy(): Returns a copy of the Results object with all tensors as numpy arrays.\n        cuda(): Returns a copy of the Results object with all tensors on GPU memory.\n        to(*args, **kwargs): Returns a copy of the Results object with tensors on a specified device and dtype.\n        new(): Returns a new Results object with the same image, path, and names.\n        plot(...): Plots detection results on an input image, returning an annotated image.\n        show(): Show annotated results to screen.\n        save(filename): Save annotated results to file.\n        verbose(): Returns a log string for each task, detailing detections and classifications.\n        save_txt(txt_file, save_conf=False): Saves detection results to a text file.\n        save_crop(save_dir, file_name=Path(\"im.jpg\")): Saves cropped detection images.\n        tojson(normalize=False): Converts detection results to JSON format.\n    "
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "\n",
    "for result in results:\n",
    "    if i > 5:\n",
    "        break\n",
    "\n",
    "    # boxes = result.boxes  # Boxes object for bounding box outputs\n",
    "    # masks = result.masks  # Masks object for segmentation masks outputs\n",
    "    # keypoints = result.keypoints  # Keypoints object for pose outputs\n",
    "    # probs = result.probs  # Probs object for classification outputs\n",
    "\n",
    "    # print('boxes \\n')\n",
    "    # print(boxes.data)\n",
    "\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49207387-bcee-4bca-92eb-70f7d7a3ea91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2edfb0bd-8eda-4244-8d16-6790c77bcc84",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO('yolov8n.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bf84b017-7f42-4652-a844-68dd059b5a59",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "video capture\n",
      "frame 0 read succeeded\n",
      "predicting\n",
      "\n",
      "0: 384x640 13 persons, 15 cars, 5 motorcycles, 2 buss, 5.5ms\n",
      "Speed: 1.9ms preprocess, 5.5ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "frame 1 read succeeded\n",
      "predicting\n",
      "\n",
      "0: 384x640 13 persons, 15 cars, 3 motorcycles, 2 buss, 5.2ms\n",
      "Speed: 1.0ms preprocess, 5.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "frame 2 read succeeded\n",
      "predicting\n",
      "\n",
      "0: 384x640 15 persons, 16 cars, 3 motorcycles, 2 buss, 5.1ms\n",
      "Speed: 1.0ms preprocess, 5.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "frame 3 read succeeded\n",
      "predicting\n",
      "\n",
      "0: 384x640 14 persons, 14 cars, 3 motorcycles, 2 buss, 5.3ms\n",
      "Speed: 1.1ms preprocess, 5.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "frame 4 read succeeded\n",
      "predicting\n",
      "\n",
      "0: 384x640 13 persons, 14 cars, 2 motorcycles, 2 buss, 5.3ms\n",
      "Speed: 1.0ms preprocess, 5.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "frame 5 read succeeded\n",
      "predicting\n",
      "\n",
      "0: 384x640 13 persons, 14 cars, 2 motorcycles, 2 buss, 5.2ms\n",
      "Speed: 1.0ms preprocess, 5.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "frame 6 read succeeded\n",
      "predicting\n",
      "\n",
      "0: 384x640 13 persons, 15 cars, 3 motorcycles, 2 buss, 5.2ms\n",
      "Speed: 1.0ms preprocess, 5.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "frame 7 read succeeded\n",
      "predicting\n",
      "\n",
      "0: 384x640 11 persons, 15 cars, 3 motorcycles, 2 buss, 5.3ms\n",
      "Speed: 1.0ms preprocess, 5.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "frame 8 read succeeded\n",
      "predicting\n",
      "\n",
      "0: 384x640 11 persons, 15 cars, 3 motorcycles, 2 buss, 5.2ms\n",
      "Speed: 1.0ms preprocess, 5.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "frame 9 read succeeded\n",
      "predicting\n",
      "\n",
      "0: 384x640 11 persons, 15 cars, 2 motorcycles, 2 buss, 5.4ms\n",
      "Speed: 1.0ms preprocess, 5.4ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "frame 10 read succeeded\n",
      "predicting\n",
      "\n",
      "0: 384x640 12 persons, 15 cars, 3 motorcycles, 1 bus, 5.2ms\n",
      "Speed: 1.0ms preprocess, 5.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "frame 11 read succeeded\n",
      "predicting\n",
      "\n",
      "0: 384x640 12 persons, 16 cars, 2 motorcycles, 1 bus, 5.3ms\n",
      "Speed: 1.0ms preprocess, 5.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "frame 12 read succeeded\n",
      "predicting\n",
      "\n",
      "0: 384x640 13 persons, 16 cars, 2 motorcycles, 1 bus, 5.2ms\n",
      "Speed: 1.0ms preprocess, 5.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "frame 13 read succeeded\n",
      "predicting\n",
      "\n",
      "0: 384x640 13 persons, 17 cars, 3 motorcycles, 2 buss, 5.3ms\n",
      "Speed: 1.0ms preprocess, 5.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "frame 14 read succeeded\n",
      "predicting\n",
      "\n",
      "0: 384x640 11 persons, 18 cars, 5 motorcycles, 1 bus, 5.1ms\n",
      "Speed: 1.0ms preprocess, 5.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "frame 15 read succeeded\n",
      "predicting\n",
      "\n",
      "0: 384x640 11 persons, 18 cars, 4 motorcycles, 1 bus, 1 truck, 5.3ms\n",
      "Speed: 1.0ms preprocess, 5.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "frame 16 read succeeded\n",
      "predicting\n",
      "\n",
      "0: 384x640 12 persons, 19 cars, 4 motorcycles, 1 bus, 5.3ms\n",
      "Speed: 1.0ms preprocess, 5.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "frame 17 read succeeded\n",
      "predicting\n",
      "\n",
      "0: 384x640 13 persons, 18 cars, 3 motorcycles, 1 bus, 5.1ms\n",
      "Speed: 1.0ms preprocess, 5.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "frame 18 read succeeded\n",
      "predicting\n",
      "\n",
      "0: 384x640 14 persons, 20 cars, 5 motorcycles, 1 bus, 1 truck, 5.1ms\n",
      "Speed: 1.0ms preprocess, 5.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "frame 19 read succeeded\n",
      "predicting\n",
      "\n",
      "0: 384x640 13 persons, 21 cars, 3 motorcycles, 1 bus, 1 truck, 5.3ms\n",
      "Speed: 1.0ms preprocess, 5.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "frame 20 read succeeded\n",
      "predicting\n",
      "\n",
      "0: 384x640 12 persons, 19 cars, 3 motorcycles, 1 bus, 1 truck, 5.1ms\n",
      "Speed: 1.0ms preprocess, 5.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "tensor([[0.0000e+00, 6.1258e+02, 6.1853e+02,  ..., 7.1939e+02, 8.9658e-01, 2.0000e+00],\n",
      "        [0.0000e+00, 8.1291e+02, 4.8729e+02,  ..., 5.9863e+02, 8.1116e-01, 2.0000e+00],\n",
      "        [0.0000e+00, 7.0963e+02, 5.1519e+02,  ..., 6.5955e+02, 8.0754e-01, 2.0000e+00],\n",
      "        ...,\n",
      "        [2.0000e+01, 1.0240e+03, 2.7922e+02,  ..., 3.1335e+02, 2.8361e-01, 2.0000e+00],\n",
      "        [2.0000e+01, 1.2334e+03, 3.5734e+02,  ..., 3.9853e+02, 2.8268e-01, 2.0000e+00],\n",
      "        [2.0000e+01, 9.9365e+02, 5.1800e+02,  ..., 6.8353e+02, 2.5574e-01, 7.0000e+00]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "video_path = '/raid/tungks/shared_drive_cv/GTTMData/external/youtube/videos/nga-tu-so_cutted.mp4'\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "print('video capture')\n",
    "i = 0\n",
    "\n",
    "while cap.isOpened():\n",
    "    if i > 20:\n",
    "        break\n",
    "    success, frame = cap.read()\n",
    "   \n",
    "    if success:\n",
    "        print(f'frame {i} read succeeded')\n",
    "        cv2.imwrite(f'my_video_frame_{i}.png', frame)\n",
    "        print('predicting')\n",
    "        results = model.predict(frame)\n",
    "\n",
    "        tensor = results[0].boxes.data\n",
    "        zeros_column = torch.zeros(tensor.shape[0], 1, device=tensor.device)\n",
    "        zeros_column.fill_(i)\n",
    "        new_tensor = torch.cat((zeros_column, tensor), dim=1)\n",
    "        if i == 0:\n",
    "            stacked_tensor = new_tensor\n",
    "        else:\n",
    "            stacked_tensor = torch.cat((stacked_tensor, new_tensor), dim=0)\n",
    "        \n",
    "        i += 1\n",
    "\n",
    "print(stacked_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3fca9aa2-1697-4e1e-a4f5-1c68b0cd0704",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([199, 7])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacked_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524579e0-dcb7-4aef-923f-328ebb7cd312",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CV",
   "language": "python",
   "name": "cv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
